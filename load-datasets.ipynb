{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "523d46b4",
   "metadata": {},
   "source": [
    "# Kaggle Skincare Dataset Manager\n",
    "## Setup Instructions\n",
    "\n",
    "Before running the main code, you need to authenticate with Kaggle API. Choose one method below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333220d3",
   "metadata": {},
   "source": [
    "## Method 1: Using kaggle.json (Recommended)\n",
    "\n",
    "1. Go to https://www.kaggle.com/settings/account\n",
    "2. Click \"Create New API Token\" - this downloads `kaggle.json`\n",
    "3. Save it to: `C:\\Users\\B H\\.kaggle\\kaggle.json`\n",
    "4. Make sure the folder has proper permissions\n",
    "\n",
    "## Method 2: Using Environment Variables\n",
    "\n",
    "If you don't have kaggle.json, use the cell below to set environment variables instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9103f8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Kaggle authentication configured!\n",
      "   Method: Environment Variables\n",
      "   Username: Marwa-001\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================\n",
    "# KAGGLE AUTHENTICATION SETUP\n",
    "# ============================================\n",
    "# Uncomment ONE of the methods below:\n",
    "\n",
    "# METHOD 1: Using kaggle.json file\n",
    "# (No code needed - it will auto-detect from C:\\Users\\B H\\.kaggle\\kaggle.json)\n",
    "\n",
    "# METHOD 2: Using Environment Variables\n",
    "# Replace with your actual Kaggle username and API key\n",
    "# Get these from: https://www.kaggle.com/settings/account\n",
    "\n",
    "os.environ['KAGGLE_USERNAME'] = 'Marwa-001'\n",
    "os.environ['KAGGLE_KEY'] = 'YOUR_KAGGLE_API_KEY'\n",
    "\n",
    "print(\"‚úÖ Kaggle authentication configured!\")\n",
    "print(f\"   Method: Environment Variables\")\n",
    "print(f\"   Username: {os.environ.get('KAGGLE_USERNAME', 'Not set')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec5d6fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Skincare Dataset Downloader & Merger\n",
      "============================================================\n",
      "‚úÖ Authenticated to Kaggle using environment variables\n",
      "\n",
      "Step 1: Downloading datasets from Kaggle...\n",
      "‚ö†Ô∏è  Make sure you have kaggle.json configured in ~/.kaggle/ or set KAGGLE_USERNAME/KAGGLE_KEY env vars\n",
      "üîΩ Downloading datasets from Kaggle...\n",
      "\n",
      "üì¶ Downloading sephora...\n",
      "Dataset URL: https://www.kaggle.com/datasets/raghadalharbi/all-products-available-on-sephora-website\n",
      "Downloading all-products-available-on-sephora-website.zip to skincare_datasets\\sephora\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.64M/4.64M [00:04<00:00, 982kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ sephora downloaded successfully!\n",
      "\n",
      "üì¶ Downloading amazon_beauty...\n",
      "Dataset URL: https://www.kaggle.com/datasets/skillsmuggler/amazon-ratings\n",
      "Downloading amazon-ratings.zip to skincare_datasets\\amazon_beauty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.8M/28.8M [00:31<00:00, 970kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ amazon_beauty downloaded successfully!\n",
      "\n",
      "üì¶ Downloading cosmetics...\n",
      "Dataset URL: https://www.kaggle.com/datasets/kingabzpro/cosmetics-datasets\n",
      "Downloading cosmetics-datasets.zip to skincare_datasets\\cosmetics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 263k/263k [00:00<00:00, 296kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ cosmetics downloaded successfully!\n",
      "\n",
      "üì¶ Downloading skincare_reviews...\n",
      "Dataset URL: https://www.kaggle.com/datasets/mrmars1010/skincare-reviews\n",
      "‚ùå Error downloading skincare_reviews: 403 Client Error: Forbidden for url: https://api.kaggle.com/v1/datasets.DatasetApiService/DownloadDataset\n",
      "   Please manually download from: https://www.kaggle.com/datasets/mrmars1010/skincare-reviews\n",
      "\n",
      "üì¶ Downloading makeup_products...\n",
      "Dataset URL: https://www.kaggle.com/datasets/shudhanshusingh/25000-makeup-products-with-ingredients\n",
      "‚ùå Error downloading makeup_products: 403 Client Error: Forbidden for url: https://api.kaggle.com/v1/datasets.DatasetApiService/DownloadDataset\n",
      "   Please manually download from: https://www.kaggle.com/datasets/shudhanshusingh/25000-makeup-products-with-ingredients\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìä Loading and exploring datasets...\n",
      "\n",
      "üìÅ SEPHORA:\n",
      "   File: sephora_website_dataset.csv\n",
      "   Columns: ['id', 'brand', 'category', 'name', 'size', 'rating', 'number_of_reviews', 'love', 'price', 'value_price', 'URL', 'MarketingFlags', 'MarketingFlags_content', 'options', 'details', 'how_to_use', 'ingredients', 'online_only', 'exclusive', 'limited_edition', 'limited_time_offer']\n",
      "   Shape: (5, 21)\n",
      "\n",
      "üìÅ AMAZON_BEAUTY:\n",
      "   File: ratings_Beauty.csv\n",
      "   Columns: ['UserId', 'ProductId', 'Rating', 'Timestamp']\n",
      "   Shape: (5, 4)\n",
      "\n",
      "üìÅ COSMETICS:\n",
      "   File: cosmetics.csv\n",
      "   Columns: ['Label', 'Brand', 'Name', 'Price', 'Rank', 'Ingredients', 'Combination', 'Dry', 'Normal', 'Oily', 'Sensitive']\n",
      "   Shape: (5, 11)\n",
      "\n",
      "============================================================\n",
      "\n",
      "üî® Creating master product dataset...\n",
      "‚úÖ Processed Sephora: 9168 products\n",
      "‚úÖ Processed Cosmetics: 1472 products\n",
      "\n",
      "‚ú® Master product dataset created!\n",
      "   Total products: 9538\n",
      "   Saved to: skincare_datasets\\master_products.csv\n",
      "\n",
      "============================================================\n",
      "\n",
      "üî® Creating master reviews dataset...\n",
      "‚úÖ Processed Amazon reviews: 50000 reviews\n",
      "\n",
      "‚ú® Master reviews dataset created!\n",
      "   Total reviews: 0\n",
      "   Saved to: skincare_datasets\\master_reviews.csv\n",
      "\n",
      "============================================================\n",
      "\n",
      "üî® Creating ingredient database...\n",
      "‚ú® Ingredient database created!\n",
      "   Unique ingredients: 10791\n",
      "   Saved to: skincare_datasets\\ingredient_database.csv\n",
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üìã DATASET SUMMARY REPORT\n",
      "============================================================\n",
      "\n",
      "üìä Master Products:\n",
      "   Rows: 9,538\n",
      "   Columns: 7\n",
      "   File size: 9.87 MB\n",
      "   Location: skincare_datasets\\master_products.csv\n",
      "\n",
      "üìä Master Reviews:\n",
      "   Rows: 0\n",
      "   Columns: 8\n",
      "   File size: 0.00 MB\n",
      "   Location: skincare_datasets\\master_reviews.csv\n",
      "\n",
      "üìä Ingredient Database:\n",
      "   Rows: 10,791\n",
      "   Columns: 3\n",
      "   File size: 8.57 MB\n",
      "   Location: skincare_datasets\\ingredient_database.csv\n",
      "\n",
      "============================================================\n",
      "‚úÖ Dataset preparation complete!\n",
      "============================================================\n",
      "\n",
      "üéâ All done! Your datasets are ready for ML training.\n",
      "\n",
      "Next steps:\n",
      "1. Review the master datasets in the 'skincare_datasets' folder\n",
      "2. Run data cleaning and preprocessing\n",
      "3. Create feature engineering pipeline\n",
      "4. Train your ML models\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "class SkincareDatasetManager:\n",
    "    def __init__(self, output_dir='skincare_datasets'):\n",
    "        \"\"\"Initialize the dataset manager\"\"\"\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # Initialize Kaggle API safely (do NOT import kaggle at module import time)\n",
    "        self.api = None\n",
    "\n",
    "        # Prefer environment variables if set\n",
    "        kaggle_username = os.environ.get('KAGGLE_USERNAME')\n",
    "        kaggle_key = os.environ.get('KAGGLE_KEY')\n",
    "        kaggle_json = Path.home() / '.kaggle' / 'kaggle.json'\n",
    "\n",
    "        if kaggle_username and kaggle_key:\n",
    "            try:\n",
    "                from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "                self.api = KaggleApi()\n",
    "                self.api.authenticate()\n",
    "                print(\"‚úÖ Authenticated to Kaggle using environment variables\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Kaggle authentication (env) failed: {e}\")\n",
    "                self.api = None\n",
    "        elif kaggle_json.exists():\n",
    "            try:\n",
    "                from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "                self.api = KaggleApi()\n",
    "                self.api.authenticate()\n",
    "                print(f\"‚úÖ Authenticated to Kaggle using {kaggle_json}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Kaggle authentication (kaggle.json) failed: {e}\")\n",
    "                self.api = None\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Could not find Kaggle credentials. Dataset download will be skipped unless you configure auth.\")\n",
    "            print(\"   See: https://github.com/Kaggle/kaggle-api/\")\n",
    "            self.api = None\n",
    "\n",
    "        # Dataset sources from Kaggle\n",
    "        self.kaggle_datasets = {\n",
    "            'sephora': 'raghadalharbi/all-products-available-on-sephora-website',\n",
    "            'amazon_beauty': 'skillsmuggler/amazon-ratings',\n",
    "            'cosmetics': 'kingabzpro/cosmetics-datasets',\n",
    "            'skincare_reviews': 'mrmars1010/skincare-reviews',\n",
    "            'makeup_products': 'shudhanshusingh/25000-makeup-products-with-ingredients'\n",
    "        }\n",
    "    \n",
    "    def download_kaggle_datasets(self):\n",
    "        \"\"\"Download datasets from Kaggle\"\"\"\n",
    "        if self.api is None:\n",
    "            print(\"‚ö†Ô∏è  Kaggle API not configured ‚Äî skipping downloads. Configure kaggle.json or environment variables and re-run the auth cell.\")\n",
    "            return\n",
    "\n",
    "        print(\"üîΩ Downloading datasets from Kaggle...\")\n",
    "        \n",
    "        for name, dataset_path in self.kaggle_datasets.items():\n",
    "            try:\n",
    "                dataset_dir = self.output_dir / name\n",
    "                dataset_dir.mkdir(exist_ok=True)\n",
    "                \n",
    "                print(f\"\\nüì¶ Downloading {name}...\")\n",
    "                self.api.dataset_download_files(\n",
    "                    dataset_path,\n",
    "                    path=str(dataset_dir),\n",
    "                    unzip=True,\n",
    "                    quiet=False\n",
    "                )\n",
    "                print(f\"‚úÖ {name} downloaded successfully!\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error downloading {name}: {str(e)}\")\n",
    "                print(f\"   Please manually download from: https://www.kaggle.com/datasets/{dataset_path}\")\n",
    "    \n",
    "    def load_and_explore_datasets(self):\n",
    "        \"\"\"Load all downloaded datasets and show their structure\"\"\"\n",
    "        print(\"\\nüìä Loading and exploring datasets...\")\n",
    "        \n",
    "        datasets = {}\n",
    "        \n",
    "        for name in self.kaggle_datasets.keys():\n",
    "            dataset_dir = self.output_dir / name\n",
    "            if not dataset_dir.exists():\n",
    "                print(f\"‚ö†Ô∏è  {name} directory not found. Skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Find CSV files in the directory\n",
    "            csv_files = list(dataset_dir.glob('*.csv'))\n",
    "            \n",
    "            if csv_files:\n",
    "                print(f\"\\nüìÅ {name.upper()}:\")\n",
    "                for csv_file in csv_files:\n",
    "                    try:\n",
    "                        df = pd.read_csv(csv_file, nrows=5)  # Load first 5 rows to preview\n",
    "                        print(f\"   File: {csv_file.name}\")\n",
    "                        print(f\"   Columns: {list(df.columns)}\")\n",
    "                        print(f\"   Shape: {df.shape}\")\n",
    "                        datasets[f\"{name}_{csv_file.stem}\"] = csv_file\n",
    "                    except Exception as e:\n",
    "                        print(f\"   ‚ùå Error reading {csv_file.name}: {str(e)}\")\n",
    "        \n",
    "        return datasets\n",
    "    \n",
    "    def create_master_product_dataset(self):\n",
    "        \"\"\"Merge product datasets into a master product database\"\"\"\n",
    "        print(\"\\nüî® Creating master product dataset...\")\n",
    "        \n",
    "        all_products = []\n",
    "        \n",
    "        # Process Sephora dataset\n",
    "        sephora_path = self.output_dir / 'sephora'\n",
    "        if sephora_path.exists():\n",
    "            for csv_file in sephora_path.glob('*.csv'):\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_file)\n",
    "                    \n",
    "                    # Standardize column names\n",
    "                    product_df = pd.DataFrame({\n",
    "                        'product_name': df.get('product_name', df.get('name', df.get('Product', ''))),\n",
    "                        'brand': df.get('brand_name', df.get('brand', df.get('Brand', ''))),\n",
    "                        'category': df.get('category', df.get('primary_category', '')),\n",
    "                        'ingredients': df.get('ingredients', df.get('ingredient_list', '')),\n",
    "                        'price': df.get('price', df.get('price_usd', np.nan)),\n",
    "                        'rating': df.get('rating', df.get('reviews', np.nan)),\n",
    "                        'source': 'sephora'\n",
    "                    })\n",
    "                    \n",
    "                    all_products.append(product_df)\n",
    "                    print(f\"‚úÖ Processed Sephora: {len(product_df)} products\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error processing Sephora file: {str(e)}\")\n",
    "        \n",
    "        # Process Cosmetics dataset\n",
    "        cosmetics_path = self.output_dir / 'cosmetics'\n",
    "        if cosmetics_path.exists():\n",
    "            for csv_file in cosmetics_path.glob('*.csv'):\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_file)\n",
    "                    \n",
    "                    product_df = pd.DataFrame({\n",
    "                        'product_name': df.get('Label', df.get('name', df.get('product_name', ''))),\n",
    "                        'brand': df.get('Brand', df.get('brand', '')),\n",
    "                        'category': df.get('Category', df.get('category', '')),\n",
    "                        'ingredients': df.get('Ingredients', df.get('ingredients', '')),\n",
    "                        'price': df.get('Price', df.get('price', np.nan)),\n",
    "                        'rating': np.nan,\n",
    "                        'source': 'cosmetics'\n",
    "                    })\n",
    "                    \n",
    "                    all_products.append(product_df)\n",
    "                    print(f\"‚úÖ Processed Cosmetics: {len(product_df)} products\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error processing Cosmetics file: {str(e)}\")\n",
    "        \n",
    "        # Process Makeup Products dataset\n",
    "        makeup_path = self.output_dir / 'makeup_products'\n",
    "        if makeup_path.exists():\n",
    "            for csv_file in makeup_path.glob('*.csv'):\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_file)\n",
    "                    \n",
    "                    product_df = pd.DataFrame({\n",
    "                        'product_name': df.get('product_name', df.get('name', '')),\n",
    "                        'brand': df.get('brand', df.get('brand_name', '')),\n",
    "                        'category': df.get('product_type', df.get('category', '')),\n",
    "                        'ingredients': df.get('ingredients', ''),\n",
    "                        'price': df.get('price', np.nan),\n",
    "                        'rating': df.get('rating', np.nan),\n",
    "                        'source': 'makeup'\n",
    "                    })\n",
    "                    \n",
    "                    all_products.append(product_df)\n",
    "                    print(f\"‚úÖ Processed Makeup: {len(product_df)} products\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error processing Makeup file: {str(e)}\")\n",
    "        \n",
    "        # Combine all products\n",
    "        if all_products:\n",
    "            master_products = pd.concat(all_products, ignore_index=True)\n",
    "            \n",
    "            # Clean and deduplicate\n",
    "            master_products = master_products.drop_duplicates(subset=['product_name', 'brand'])\n",
    "            master_products = master_products[master_products['product_name'].notna()]\n",
    "            \n",
    "            # Save master product dataset\n",
    "            output_path = self.output_dir / 'master_products.csv'\n",
    "            master_products.to_csv(output_path, index=False)\n",
    "            \n",
    "            print(f\"\\n‚ú® Master product dataset created!\")\n",
    "            print(f\"   Total products: {len(master_products)}\")\n",
    "            print(f\"   Saved to: {output_path}\")\n",
    "            \n",
    "            return master_products\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No product data found to merge\")\n",
    "            return None\n",
    "    \n",
    "    def create_master_reviews_dataset(self):\n",
    "        \"\"\"Merge review datasets into a master reviews database\"\"\"\n",
    "        print(\"\\nüî® Creating master reviews dataset...\")\n",
    "        \n",
    "        all_reviews = []\n",
    "        \n",
    "        # Process Amazon Beauty reviews\n",
    "        amazon_path = self.output_dir / 'amazon_beauty'\n",
    "        if amazon_path.exists():\n",
    "            for csv_file in amazon_path.glob('*.csv'):\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_file, nrows=50000)  # Limit to 50k reviews per file\n",
    "                    \n",
    "                    reviews_df = pd.DataFrame({\n",
    "                        'product_name': df.get('ProductId', df.get('product_name', '')),\n",
    "                        'user_id': df.get('UserId', df.get('user_id', '')),\n",
    "                        'rating': df.get('Score', df.get('rating', df.get('Rating', np.nan))),\n",
    "                        'review_text': df.get('Text', df.get('review', df.get('review_text', ''))),\n",
    "                        'review_summary': df.get('Summary', df.get('summary', '')),\n",
    "                        'helpful_votes': df.get('HelpfulnessNumerator', 0),\n",
    "                        'timestamp': df.get('Time', df.get('timestamp', '')),\n",
    "                        'source': 'amazon'\n",
    "                    })\n",
    "                    \n",
    "                    all_reviews.append(reviews_df)\n",
    "                    print(f\"‚úÖ Processed Amazon reviews: {len(reviews_df)} reviews\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error processing Amazon reviews: {str(e)}\")\n",
    "        \n",
    "        # Process Skincare Reviews\n",
    "        skincare_reviews_path = self.output_dir / 'skincare_reviews'\n",
    "        if skincare_reviews_path.exists():\n",
    "            for csv_file in skincare_reviews_path.glob('*.csv'):\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_file)\n",
    "                    \n",
    "                    reviews_df = pd.DataFrame({\n",
    "                        'product_name': df.get('product_name', df.get('Product', '')),\n",
    "                        'user_id': df.get('author', df.get('user_id', '')),\n",
    "                        'rating': df.get('rating', df.get('Rating', np.nan)),\n",
    "                        'review_text': df.get('review_text', df.get('review', '')),\n",
    "                        'review_summary': df.get('review_title', ''),\n",
    "                        'helpful_votes': df.get('helpful_count', 0),\n",
    "                        'timestamp': df.get('date', df.get('timestamp', '')),\n",
    "                        'source': 'skincare_reviews'\n",
    "                    })\n",
    "                    \n",
    "                    all_reviews.append(reviews_df)\n",
    "                    print(f\"‚úÖ Processed skincare reviews: {len(reviews_df)} reviews\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error processing skincare reviews: {str(e)}\")\n",
    "        \n",
    "        # Combine all reviews\n",
    "        if all_reviews:\n",
    "            master_reviews = pd.concat(all_reviews, ignore_index=True)\n",
    "            \n",
    "            # Clean data\n",
    "            master_reviews = master_reviews[master_reviews['review_text'].notna()]\n",
    "            master_reviews['review_text'] = master_reviews['review_text'].astype(str)\n",
    "            master_reviews = master_reviews[master_reviews['review_text'].str.len() > 10]\n",
    "            \n",
    "            # Save master reviews dataset\n",
    "            output_path = self.output_dir / 'master_reviews.csv'\n",
    "            master_reviews.to_csv(output_path, index=False)\n",
    "            \n",
    "            print(f\"\\n‚ú® Master reviews dataset created!\")\n",
    "            print(f\"   Total reviews: {len(master_reviews)}\")\n",
    "            print(f\"   Saved to: {output_path}\")\n",
    "            \n",
    "            return master_reviews\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No review data found to merge\")\n",
    "            return None\n",
    "    \n",
    "    def create_ingredient_database(self, products_df):\n",
    "        \"\"\"Extract and create ingredient database from products\"\"\"\n",
    "        print(\"\\nüî® Creating ingredient database...\")\n",
    "        \n",
    "        if products_df is None or 'ingredients' not in products_df.columns:\n",
    "            print(\"‚ö†Ô∏è  No product data with ingredients available\")\n",
    "            return None\n",
    "        \n",
    "        all_ingredients = []\n",
    "        \n",
    "        for idx, row in products_df.iterrows():\n",
    "            if pd.notna(row['ingredients']):\n",
    "                ingredients_text = str(row['ingredients'])\n",
    "                \n",
    "                # Split ingredients (common separators)\n",
    "                ingredients = re.split(r'[,;]', ingredients_text)\n",
    "                \n",
    "                for ingredient in ingredients:\n",
    "                    ingredient = ingredient.strip()\n",
    "                    if ingredient and len(ingredient) > 2:\n",
    "                        all_ingredients.append({\n",
    "                            'ingredient_name': ingredient,\n",
    "                            'product_name': row['product_name'],\n",
    "                            'brand': row['brand'],\n",
    "                            'category': row['category']\n",
    "                        })\n",
    "        \n",
    "        if all_ingredients:\n",
    "            ingredients_df = pd.DataFrame(all_ingredients)\n",
    "            \n",
    "            # Create ingredient frequency table\n",
    "            ingredient_stats = ingredients_df.groupby('ingredient_name').agg({\n",
    "                'product_name': 'count',\n",
    "                'category': lambda x: x.mode()[0] if len(x.mode()) > 0 else 'unknown'\n",
    "            }).reset_index()\n",
    "            \n",
    "            ingredient_stats.columns = ['ingredient_name', 'frequency', 'common_category']\n",
    "            ingredient_stats = ingredient_stats.sort_values('frequency', ascending=False)\n",
    "            \n",
    "            # Save ingredient database\n",
    "            output_path = self.output_dir / 'ingredient_database.csv'\n",
    "            ingredient_stats.to_csv(output_path, index=False)\n",
    "            \n",
    "            print(f\"‚ú® Ingredient database created!\")\n",
    "            print(f\"   Unique ingredients: {len(ingredient_stats)}\")\n",
    "            print(f\"   Saved to: {output_path}\")\n",
    "            \n",
    "            return ingredient_stats\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No ingredients extracted\")\n",
    "            return None\n",
    "    \n",
    "    def generate_summary_report(self):\n",
    "        \"\"\"Generate a summary report of all datasets\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìã DATASET SUMMARY REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        files = {\n",
    "            'Master Products': 'master_products.csv',\n",
    "            'Master Reviews': 'master_reviews.csv',\n",
    "            'Ingredient Database': 'ingredient_database.csv'\n",
    "        }\n",
    "        \n",
    "        for name, filename in files.items():\n",
    "            filepath = self.output_dir / filename\n",
    "            if filepath.exists():\n",
    "                df = pd.read_csv(filepath)\n",
    "                print(f\"\\nüìä {name}:\")\n",
    "                print(f\"   Rows: {len(df):,}\")\n",
    "                print(f\"   Columns: {len(df.columns)}\")\n",
    "                print(f\"   File size: {filepath.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "                print(f\"   Location: {filepath}\")\n",
    "            else:\n",
    "                print(f\"\\n‚ö†Ô∏è  {name}: Not found\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"‚úÖ Dataset preparation complete!\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"üöÄ Skincare Dataset Downloader & Merger\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Initialize manager\n",
    "    manager = SkincareDatasetManager()\n",
    "    \n",
    "    # Step 1: Download datasets\n",
    "    print(\"\\nStep 1: Downloading datasets from Kaggle...\")\n",
    "    print(\"‚ö†Ô∏è  Make sure you have kaggle.json configured in ~/.kaggle/ or set KAGGLE_USERNAME/KAGGLE_KEY env vars\")\n",
    "    manager.download_kaggle_datasets()\n",
    "    \n",
    "    # Step 2: Explore datasets\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    datasets = manager.load_and_explore_datasets()\n",
    "    \n",
    "    # Step 3: Create master product dataset\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    products_df = manager.create_master_product_dataset()\n",
    "    \n",
    "    # Step 4: Create master reviews dataset\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    reviews_df = manager.create_master_reviews_dataset()\n",
    "    \n",
    "    # Step 5: Create ingredient database\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    ingredients_df = manager.create_ingredient_database(products_df)\n",
    "    \n",
    "    # Step 6: Generate summary report\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    manager.generate_summary_report()\n",
    "    \n",
    "    print(\"\\nüéâ All done! Your datasets are ready for ML training.\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Review the master datasets in the 'skincare_datasets' folder\")\n",
    "    print(\"2. Run data cleaning and preprocessing\")\n",
    "    print(\"3. Create feature engineering pipeline\")\n",
    "    print(\"4. Train your ML models\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece272a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
