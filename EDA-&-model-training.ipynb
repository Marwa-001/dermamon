{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ab27e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f9eb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = pd.read_csv('skincare_datasets/master_products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c31c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand</th>\n",
       "      <th>category</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>price</th>\n",
       "      <th>rating</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Blu Mediterraneo MINIATURE Set</td>\n",
       "      <td>Acqua Di Parma</td>\n",
       "      <td>Fragrance</td>\n",
       "      <td>Arancia di Capri Eau de Toilette: Alcohol Dena...</td>\n",
       "      <td>66.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>sephora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Colonia</td>\n",
       "      <td>Acqua Di Parma</td>\n",
       "      <td>Cologne</td>\n",
       "      <td>unknown</td>\n",
       "      <td>66.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>sephora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arancia di Capri</td>\n",
       "      <td>Acqua Di Parma</td>\n",
       "      <td>Perfume</td>\n",
       "      <td>Alcohol Denat.- Water- Fragrance- Limonene- Li...</td>\n",
       "      <td>180.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>sephora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mirto di Panarea</td>\n",
       "      <td>Acqua Di Parma</td>\n",
       "      <td>Perfume</td>\n",
       "      <td>unknown</td>\n",
       "      <td>120.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>sephora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Colonia Miniature Set</td>\n",
       "      <td>Acqua Di Parma</td>\n",
       "      <td>Fragrance</td>\n",
       "      <td>Colonia: Alcohol Denat.- Water- Fragrance- Lim...</td>\n",
       "      <td>72.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>sephora</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     product_name           brand   category  \\\n",
       "0  Blu Mediterraneo MINIATURE Set  Acqua Di Parma  Fragrance   \n",
       "1                         Colonia  Acqua Di Parma    Cologne   \n",
       "2                Arancia di Capri  Acqua Di Parma    Perfume   \n",
       "3                Mirto di Panarea  Acqua Di Parma    Perfume   \n",
       "4           Colonia Miniature Set  Acqua Di Parma  Fragrance   \n",
       "\n",
       "                                         ingredients  price  rating   source  \n",
       "0  Arancia di Capri Eau de Toilette: Alcohol Dena...   66.0     4.0  sephora  \n",
       "1                                            unknown   66.0     4.5  sephora  \n",
       "2  Alcohol Denat.- Water- Fragrance- Limonene- Li...  180.0     4.5  sephora  \n",
       "3                                            unknown  120.0     4.5  sephora  \n",
       "4  Colonia: Alcohol Denat.- Water- Fragrance- Lim...   72.0     3.5  sephora  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee491528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Skincare Data Preprocessing & Feature Engineering Pipeline\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STEP 1: Loading Datasets\n",
      "======================================================================\n",
      "üìÇ Loading datasets...\n",
      "‚úÖ Loaded products: 9,538 rows\n",
      "‚úÖ Loaded reviews: 0 rows\n",
      "‚úÖ Loaded ingredients: 10,791 rows\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Data Cleaning\n",
      "======================================================================\n",
      "\n",
      "üßπ Cleaning products data...\n",
      "   Removed 0 duplicates\n",
      "   Removed 1660 products without ingredients\n",
      "‚úÖ Products cleaned: 7,878 products remaining\n",
      "\n",
      "üßπ Cleaning reviews data...\n",
      "   Removed 0 duplicate reviews\n",
      "   Removed 0 very short reviews\n",
      "   Removed 0 reviews without ratings\n",
      "‚úÖ Reviews cleaned: 0 reviews remaining\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Exploratory Data Analysis\n",
      "======================================================================\n",
      "\n",
      "üìä Performing Exploratory Data Analysis...\n",
      "\n",
      "üì¶ Products Analysis:\n",
      "   ‚úÖ Products visualization saved\n",
      "\n",
      "   Products Statistics:\n",
      "   - Total products: 7,878\n",
      "   - Unique brands: 388\n",
      "   - Unique categories: 127\n",
      "   - Avg price: $47.88\n",
      "   - Median price: $35.00\n",
      "   - Avg rating: 4.01\n",
      "\n",
      "üí¨ Reviews Analysis:\n",
      "   ‚úÖ Reviews visualization saved\n",
      "\n",
      "   Reviews Statistics:\n",
      "   - Total reviews: 0\n",
      "   - Avg rating: nan\n",
      "   - Median rating: nan\n",
      "   - Avg review length: nan characters\n",
      "   - Rating breakdown:\n",
      "\n",
      "‚úÖ EDA complete! Visualizations saved to skincare_datasets\\processed\\visualizations\n",
      "\n",
      "======================================================================\n",
      "STEP 4: Feature Engineering - Products\n",
      "======================================================================\n",
      "\n",
      "üß™ Extracting ingredient features...\n",
      "‚úÖ Ingredient features extracted\n",
      "   - Avg ingredients per product: 2.5\n",
      "   - Avg risk score: 27.7\n",
      "   - Risk categories:\n",
      "risk_category\n",
      "High         2428\n",
      "Low          2082\n",
      "Moderate      997\n",
      "Very High     334\n",
      "Name: count, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "STEP 5: Feature Engineering - Reviews\n",
      "======================================================================\n",
      "\n",
      "üí¨ Extracting review features...\n",
      "   Analyzing sentiment...\n",
      "‚úÖ Review features extracted\n",
      "   - Avg sentiment: nan\n",
      "   - Sentiment distribution:\n",
      "sentiment_category\n",
      "Negative    0\n",
      "Neutral     0\n",
      "Positive    0\n",
      "Name: count, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "STEP 6: Creating Feature Matrices\n",
      "======================================================================\n",
      "\n",
      "üîß Creating feature matrices...\n",
      "‚úÖ Product features: (7878, 17)\n",
      "‚úÖ Review features: (0, 15)\n",
      "\n",
      "üìä Aggregating reviews by product...\n",
      "‚úÖ Review aggregates: (0, 14)\n",
      "\n",
      "‚úÖ Feature matrices created!\n",
      "\n",
      "======================================================================\n",
      "STEP 7: Summary Report\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üìã DATA PREPROCESSING & FEATURE ENGINEERING SUMMARY\n",
      "======================================================================\n",
      "\n",
      "üìä Product Features:\n",
      "   Shape: (7878, 17)\n",
      "   Columns: product_id, product_name, brand, category, ingredient_count, risk_score, risk_category, high_risk_count...\n",
      "   File size: 0.86 MB\n",
      "   Missing values: 2414\n",
      "\n",
      "üìä Review Features:\n",
      "   Shape: (0, 15)\n",
      "   Columns: review_id, product_name, rating, sentiment_score, sentiment_category, review_length, word_count, mentions_acne...\n",
      "   File size: 0.00 MB\n",
      "   Missing values: 0\n",
      "\n",
      "üìä Review Aggregates:\n",
      "   Shape: (0, 14)\n",
      "   Columns: product_name, avg_rating, review_count, rating_std, avg_sentiment, avg_review_length, avg_word_count, mentions_acne...\n",
      "   File size: 0.00 MB\n",
      "   Missing values: 0\n",
      "\n",
      "======================================================================\n",
      "‚úÖ All processing complete! Ready for ML model training.\n",
      "======================================================================\n",
      "\n",
      "üéâ Pipeline Complete!\n",
      "\n",
      "üìÇ Check the 'skincare_datasets/processed/' folder for:\n",
      "   - product_features.csv\n",
      "   - review_features.csv\n",
      "   - review_aggregates_by_product.csv\n",
      "   - visualizations/ (EDA plots)\n",
      "\n",
      "üöÄ Next: Train ML models for risk analysis and recommendations!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Skincare Data Preprocessing, Cleaning, EDA & Feature Engineering Pipeline\n",
    "Complete pipeline for preparing skincare data for ML models\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  NLTK downloads failed. Some features may not work.\")\n",
    "\n",
    "\n",
    "class SkincareDataPreprocessor:\n",
    "    def __init__(self, data_dir='skincare_datasets'):\n",
    "        \"\"\"Initialize preprocessor\"\"\"\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.processed_dir = self.data_dir / 'processed'\n",
    "        self.processed_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Harmful/risky ingredients database\n",
    "        self.harmful_ingredients = {\n",
    "            'high_risk': [\n",
    "                'parabens', 'methylparaben', 'propylparaben', 'butylparaben',\n",
    "                'formaldehyde', 'toluene', 'phthalates', 'triclosan',\n",
    "                'hydroquinone', 'oxybenzone', 'benzophenone', 'coal tar',\n",
    "                'petrolatum', 'mineral oil', 'siloxanes', 'bha', 'bht'\n",
    "            ],\n",
    "            'moderate_risk': [\n",
    "                'sulfates', 'sls', 'sodium lauryl sulfate', 'sodium laureth sulfate',\n",
    "                'fragrance', 'parfum', 'alcohol denat', 'denatured alcohol',\n",
    "                'peg compounds', 'dmdm hydantoin', 'quaternium-15'\n",
    "            ],\n",
    "            'comedogenic': [\n",
    "                'coconut oil', 'cocoa butter', 'isopropyl myristate',\n",
    "                'isopropyl palmitate', 'acetylated lanolin', 'algae extract'\n",
    "            ],\n",
    "            'irritants': [\n",
    "                'menthol', 'camphor', 'eucalyptus', 'peppermint oil',\n",
    "                'lemon', 'lime', 'grapefruit', 'witch hazel', 'sd alcohol'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Beneficial ingredients\n",
    "        self.beneficial_ingredients = {\n",
    "            'anti_aging': [\n",
    "                'retinol', 'retinoid', 'peptides', 'vitamin c', 'ascorbic acid',\n",
    "                'hyaluronic acid', 'niacinamide', 'coenzyme q10', 'resveratrol'\n",
    "            ],\n",
    "            'moisturizing': [\n",
    "                'glycerin', 'ceramides', 'squalane', 'shea butter',\n",
    "                'jojoba oil', 'argan oil', 'aloe vera'\n",
    "            ],\n",
    "            'acne_fighting': [\n",
    "                'salicylic acid', 'benzoyl peroxide', 'tea tree oil',\n",
    "                'niacinamide', 'zinc', 'sulfur'\n",
    "            ],\n",
    "            'brightening': [\n",
    "                'vitamin c', 'kojic acid', 'arbutin', 'licorice extract',\n",
    "                'alpha arbutin', 'tranexamic acid'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def load_datasets(self):\n",
    "        \"\"\"Load all master datasets\"\"\"\n",
    "        print(\"üìÇ Loading datasets...\")\n",
    "        \n",
    "        datasets = {}\n",
    "        \n",
    "        files = {\n",
    "            'products': 'master_products.csv',\n",
    "            'reviews': 'master_reviews.csv',\n",
    "            'ingredients': 'ingredient_database.csv'\n",
    "        }\n",
    "        \n",
    "        for name, filename in files.items():\n",
    "            filepath = self.data_dir / filename\n",
    "            if filepath.exists():\n",
    "                datasets[name] = pd.read_csv(filepath)\n",
    "                print(f\"‚úÖ Loaded {name}: {len(datasets[name]):,} rows\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  {name} not found at {filepath}\")\n",
    "                datasets[name] = None\n",
    "        \n",
    "        return datasets\n",
    "    \n",
    "    def clean_products_data(self, df):\n",
    "        \"\"\"Clean and preprocess products dataset\"\"\"\n",
    "        print(\"\\nüßπ Cleaning products data...\")\n",
    "        \n",
    "        if df is None:\n",
    "            return None\n",
    "        \n",
    "        df_clean = df.copy()\n",
    "        initial_rows = len(df_clean)\n",
    "        \n",
    "        # 1. Remove duplicates\n",
    "        df_clean = df_clean.drop_duplicates(subset=['product_name', 'brand'])\n",
    "        print(f\"   Removed {initial_rows - len(df_clean)} duplicates\")\n",
    "        \n",
    "        # 2. Clean text columns\n",
    "        text_columns = ['product_name', 'brand', 'category', 'ingredients']\n",
    "        for col in text_columns:\n",
    "            if col in df_clean.columns:\n",
    "                df_clean[col] = df_clean[col].astype(str)\n",
    "                df_clean[col] = df_clean[col].str.strip()\n",
    "                df_clean[col] = df_clean[col].replace('nan', np.nan)\n",
    "        \n",
    "        # 3. Remove products without ingredients\n",
    "        before = len(df_clean)\n",
    "        df_clean = df_clean[df_clean['ingredients'].notna()]\n",
    "        df_clean = df_clean[df_clean['ingredients'].str.len() > 20]\n",
    "        print(f\"   Removed {before - len(df_clean)} products without ingredients\")\n",
    "        \n",
    "        # 4. Clean price column\n",
    "        if 'price' in df_clean.columns:\n",
    "            df_clean['price'] = pd.to_numeric(df_clean['price'], errors='coerce')\n",
    "            df_clean['price'] = df_clean['price'].clip(lower=0, upper=1000)\n",
    "        \n",
    "        # 5. Clean rating column\n",
    "        if 'rating' in df_clean.columns:\n",
    "            df_clean['rating'] = pd.to_numeric(df_clean['rating'], errors='coerce')\n",
    "            df_clean['rating'] = df_clean['rating'].clip(lower=0, upper=5)\n",
    "        \n",
    "        # 6. Standardize category names\n",
    "        if 'category' in df_clean.columns:\n",
    "            df_clean['category'] = df_clean['category'].str.lower()\n",
    "            df_clean['category'] = df_clean['category'].fillna('uncategorized')\n",
    "        \n",
    "        # 7. Create product ID\n",
    "        df_clean['product_id'] = range(1, len(df_clean) + 1)\n",
    "        df_clean['product_id'] = 'PROD_' + df_clean['product_id'].astype(str).str.zfill(6)\n",
    "        \n",
    "        print(f\"‚úÖ Products cleaned: {len(df_clean):,} products remaining\")\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def clean_reviews_data(self, df):\n",
    "        \"\"\"Clean and preprocess reviews dataset\"\"\"\n",
    "        print(\"\\nüßπ Cleaning reviews data...\")\n",
    "        \n",
    "        if df is None:\n",
    "            return None\n",
    "        \n",
    "        df_clean = df.copy()\n",
    "        initial_rows = len(df_clean)\n",
    "        \n",
    "        # 1. Remove duplicates\n",
    "        df_clean = df_clean.drop_duplicates(subset=['product_name', 'user_id', 'review_text'])\n",
    "        print(f\"   Removed {initial_rows - len(df_clean)} duplicate reviews\")\n",
    "        \n",
    "        # 2. Clean text columns\n",
    "        df_clean['review_text'] = df_clean['review_text'].astype(str)\n",
    "        df_clean['review_text'] = df_clean['review_text'].str.strip()\n",
    "        \n",
    "        # 3. Remove very short reviews (less than 10 characters)\n",
    "        before = len(df_clean)\n",
    "        df_clean = df_clean[df_clean['review_text'].str.len() >= 10]\n",
    "        print(f\"   Removed {before - len(df_clean)} very short reviews\")\n",
    "        \n",
    "        # 4. Clean rating column\n",
    "        if 'rating' in df_clean.columns:\n",
    "            df_clean['rating'] = pd.to_numeric(df_clean['rating'], errors='coerce')\n",
    "            df_clean = df_clean[df_clean['rating'].notna()]\n",
    "            df_clean['rating'] = df_clean['rating'].clip(lower=1, upper=5)\n",
    "        \n",
    "        # 5. Remove reviews with missing ratings\n",
    "        before = len(df_clean)\n",
    "        df_clean = df_clean[df_clean['rating'].notna()]\n",
    "        print(f\"   Removed {before - len(df_clean)} reviews without ratings\")\n",
    "        \n",
    "        # 6. Convert timestamps\n",
    "        if 'timestamp' in df_clean.columns:\n",
    "            df_clean['timestamp'] = pd.to_datetime(df_clean['timestamp'], errors='coerce')\n",
    "        \n",
    "        # 7. Create review ID\n",
    "        df_clean['review_id'] = range(1, len(df_clean) + 1)\n",
    "        df_clean['review_id'] = 'REV_' + df_clean['review_id'].astype(str).str.zfill(8)\n",
    "        \n",
    "        print(f\"‚úÖ Reviews cleaned: {len(df_clean):,} reviews remaining\")\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def perform_eda(self, products_df, reviews_df):\n",
    "        \"\"\"Perform Exploratory Data Analysis\"\"\"\n",
    "        print(\"\\nüìä Performing Exploratory Data Analysis...\")\n",
    "        \n",
    "        # Create visualizations directory\n",
    "        viz_dir = self.processed_dir / 'visualizations'\n",
    "        viz_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Set style\n",
    "        sns.set_style('whitegrid')\n",
    "        plt.rcParams['figure.figsize'] = (12, 6)\n",
    "        \n",
    "        # === PRODUCTS EDA ===\n",
    "        if products_df is not None:\n",
    "            print(\"\\nüì¶ Products Analysis:\")\n",
    "            \n",
    "            # 1. Top brands\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "            \n",
    "            top_brands = products_df['brand'].value_counts().head(15)\n",
    "            axes[0, 0].barh(top_brands.index, top_brands.values, color='steelblue')\n",
    "            axes[0, 0].set_xlabel('Number of Products')\n",
    "            axes[0, 0].set_title('Top 15 Brands by Product Count')\n",
    "            axes[0, 0].invert_yaxis()\n",
    "            \n",
    "            # 2. Category distribution\n",
    "            top_categories = products_df['category'].value_counts().head(10)\n",
    "            axes[0, 1].pie(top_categories.values, labels=top_categories.index, autopct='%1.1f%%')\n",
    "            axes[0, 1].set_title('Product Categories Distribution')\n",
    "            \n",
    "            # 3. Price distribution\n",
    "            if 'price' in products_df.columns:\n",
    "                price_data = products_df['price'].dropna()\n",
    "                axes[1, 0].hist(price_data[price_data < 200], bins=50, color='green', alpha=0.7)\n",
    "                axes[1, 0].set_xlabel('Price ($)')\n",
    "                axes[1, 0].set_ylabel('Frequency')\n",
    "                axes[1, 0].set_title('Price Distribution (< $200)')\n",
    "                axes[1, 0].axvline(price_data.median(), color='red', linestyle='--', \n",
    "                                   label=f'Median: ${price_data.median():.2f}')\n",
    "                axes[1, 0].legend()\n",
    "            \n",
    "            # 4. Rating distribution\n",
    "            if 'rating' in products_df.columns:\n",
    "                rating_data = products_df['rating'].dropna()\n",
    "                axes[1, 1].hist(rating_data, bins=20, color='orange', alpha=0.7)\n",
    "                axes[1, 1].set_xlabel('Rating')\n",
    "                axes[1, 1].set_ylabel('Frequency')\n",
    "                axes[1, 1].set_title('Product Rating Distribution')\n",
    "                axes[1, 1].axvline(rating_data.mean(), color='red', linestyle='--',\n",
    "                                   label=f'Mean: {rating_data.mean():.2f}')\n",
    "                axes[1, 1].legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(viz_dir / 'products_eda.png', dpi=300, bbox_inches='tight')\n",
    "            print(f\"   ‚úÖ Products visualization saved\")\n",
    "            plt.close()\n",
    "            \n",
    "            # Print statistics\n",
    "            print(f\"\\n   Products Statistics:\")\n",
    "            print(f\"   - Total products: {len(products_df):,}\")\n",
    "            print(f\"   - Unique brands: {products_df['brand'].nunique():,}\")\n",
    "            print(f\"   - Unique categories: {products_df['category'].nunique():,}\")\n",
    "            if 'price' in products_df.columns:\n",
    "                print(f\"   - Avg price: ${products_df['price'].mean():.2f}\")\n",
    "                print(f\"   - Median price: ${products_df['price'].median():.2f}\")\n",
    "            if 'rating' in products_df.columns:\n",
    "                print(f\"   - Avg rating: {products_df['rating'].mean():.2f}\")\n",
    "        \n",
    "        # === REVIEWS EDA ===\n",
    "        if reviews_df is not None:\n",
    "            print(\"\\nüí¨ Reviews Analysis:\")\n",
    "            \n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "            \n",
    "            # 1. Rating distribution\n",
    "            rating_counts = reviews_df['rating'].value_counts().sort_index()\n",
    "            axes[0, 0].bar(rating_counts.index, rating_counts.values, color='purple', alpha=0.7)\n",
    "            axes[0, 0].set_xlabel('Rating')\n",
    "            axes[0, 0].set_ylabel('Count')\n",
    "            axes[0, 0].set_title('Review Rating Distribution')\n",
    "            axes[0, 0].set_xticks(range(1, 6))\n",
    "            \n",
    "            # 2. Review length distribution\n",
    "            review_lengths = reviews_df['review_text'].str.len()\n",
    "            axes[0, 1].hist(review_lengths[review_lengths < 1000], bins=50, color='teal', alpha=0.7)\n",
    "            axes[0, 1].set_xlabel('Review Length (characters)')\n",
    "            axes[0, 1].set_ylabel('Frequency')\n",
    "            axes[0, 1].set_title('Review Length Distribution')\n",
    "            \n",
    "            # 3. Reviews by source\n",
    "            source_counts = reviews_df['source'].value_counts()\n",
    "            axes[1, 0].pie(source_counts.values, labels=source_counts.index, autopct='%1.1f%%')\n",
    "            axes[1, 0].set_title('Reviews by Source')\n",
    "            \n",
    "            # 4. Rating vs Review Length\n",
    "            avg_length_by_rating = reviews_df.groupby('rating')['review_text'].apply(lambda x: x.str.len().mean())\n",
    "            axes[1, 1].plot(avg_length_by_rating.index, avg_length_by_rating.values, \n",
    "                           marker='o', linewidth=2, markersize=8, color='red')\n",
    "            axes[1, 1].set_xlabel('Rating')\n",
    "            axes[1, 1].set_ylabel('Avg Review Length')\n",
    "            axes[1, 1].set_title('Average Review Length by Rating')\n",
    "            axes[1, 1].set_xticks(range(1, 6))\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(viz_dir / 'reviews_eda.png', dpi=300, bbox_inches='tight')\n",
    "            print(f\"   ‚úÖ Reviews visualization saved\")\n",
    "            plt.close()\n",
    "            \n",
    "            # Print statistics\n",
    "            print(f\"\\n   Reviews Statistics:\")\n",
    "            print(f\"   - Total reviews: {len(reviews_df):,}\")\n",
    "            print(f\"   - Avg rating: {reviews_df['rating'].mean():.2f}\")\n",
    "            print(f\"   - Median rating: {reviews_df['rating'].median():.2f}\")\n",
    "            print(f\"   - Avg review length: {review_lengths.mean():.0f} characters\")\n",
    "            print(f\"   - Rating breakdown:\")\n",
    "            for rating in sorted(reviews_df['rating'].unique()):\n",
    "                count = len(reviews_df[reviews_df['rating'] == rating])\n",
    "                pct = count / len(reviews_df) * 100\n",
    "                print(f\"     {rating} stars: {count:,} ({pct:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ EDA complete! Visualizations saved to {viz_dir}\")\n",
    "    \n",
    "    def extract_ingredient_features(self, products_df):\n",
    "        \"\"\"Extract ingredient-based features\"\"\"\n",
    "        print(\"\\nüß™ Extracting ingredient features...\")\n",
    "        \n",
    "        if products_df is None:\n",
    "            return None\n",
    "        \n",
    "        df = products_df.copy()\n",
    "        \n",
    "        # Parse ingredients into lists\n",
    "        def parse_ingredients(ing_text):\n",
    "            if pd.isna(ing_text):\n",
    "                return []\n",
    "            ingredients = re.split(r'[,;]', str(ing_text).lower())\n",
    "            return [ing.strip() for ing in ingredients if len(ing.strip()) > 2]\n",
    "        \n",
    "        df['ingredient_list'] = df['ingredients'].apply(parse_ingredients)\n",
    "        df['ingredient_count'] = df['ingredient_list'].apply(len)\n",
    "        \n",
    "        # Risk scores\n",
    "        def calculate_risk_scores(ing_list):\n",
    "            scores = {\n",
    "                'high_risk_count': 0,\n",
    "                'moderate_risk_count': 0,\n",
    "                'comedogenic_count': 0,\n",
    "                'irritant_count': 0,\n",
    "                'beneficial_count': 0\n",
    "            }\n",
    "            \n",
    "            for ing in ing_list:\n",
    "                # Check harmful ingredients\n",
    "                if any(harmful in ing for harmful in self.harmful_ingredients['high_risk']):\n",
    "                    scores['high_risk_count'] += 1\n",
    "                if any(harmful in ing for harmful in self.harmful_ingredients['moderate_risk']):\n",
    "                    scores['moderate_risk_count'] += 1\n",
    "                if any(harmful in ing for harmful in self.harmful_ingredients['comedogenic']):\n",
    "                    scores['comedogenic_count'] += 1\n",
    "                if any(harmful in ing for harmful in self.harmful_ingredients['irritants']):\n",
    "                    scores['irritant_count'] += 1\n",
    "                \n",
    "                # Check beneficial ingredients\n",
    "                for benefit_type, benefit_ings in self.beneficial_ingredients.items():\n",
    "                    if any(beneficial in ing for beneficial in benefit_ings):\n",
    "                        scores['beneficial_count'] += 1\n",
    "                        break\n",
    "            \n",
    "            return pd.Series(scores)\n",
    "        \n",
    "        risk_scores = df['ingredient_list'].apply(calculate_risk_scores)\n",
    "        df = pd.concat([df, risk_scores], axis=1)\n",
    "        \n",
    "        # Calculate overall risk score (0-100)\n",
    "        df['risk_score'] = (\n",
    "            (df['high_risk_count'] * 3) + \n",
    "            (df['moderate_risk_count'] * 2) + \n",
    "            (df['comedogenic_count'] * 1.5) + \n",
    "            (df['irritant_count'] * 1)\n",
    "        ) * 10\n",
    "        df['risk_score'] = df['risk_score'].clip(upper=100)\n",
    "        \n",
    "        # Risk category\n",
    "        df['risk_category'] = pd.cut(\n",
    "            df['risk_score'],\n",
    "            bins=[0, 20, 40, 60, 100],\n",
    "            labels=['Low', 'Moderate', 'High', 'Very High']\n",
    "        )\n",
    "        \n",
    "        # Beneficial score\n",
    "        df['beneficial_score'] = (df['beneficial_count'] / df['ingredient_count'] * 100).fillna(0)\n",
    "        \n",
    "        print(f\"‚úÖ Ingredient features extracted\")\n",
    "        print(f\"   - Avg ingredients per product: {df['ingredient_count'].mean():.1f}\")\n",
    "        print(f\"   - Avg risk score: {df['risk_score'].mean():.1f}\")\n",
    "        print(f\"   - Risk categories:\")\n",
    "        print(df['risk_category'].value_counts())\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def extract_review_features(self, reviews_df):\n",
    "        \"\"\"Extract features from reviews using NLP\"\"\"\n",
    "        print(\"\\nüí¨ Extracting review features...\")\n",
    "        \n",
    "        if reviews_df is None:\n",
    "            return None\n",
    "        \n",
    "        df = reviews_df.copy()\n",
    "        \n",
    "        # 1. Sentiment analysis\n",
    "        def get_sentiment(text):\n",
    "            try:\n",
    "                blob = TextBlob(str(text))\n",
    "                return blob.sentiment.polarity\n",
    "            except:\n",
    "                return 0\n",
    "        \n",
    "        print(\"   Analyzing sentiment...\")\n",
    "        df['sentiment_score'] = df['review_text'].apply(get_sentiment)\n",
    "        df['sentiment_category'] = pd.cut(\n",
    "            df['sentiment_score'],\n",
    "            bins=[-1, -0.1, 0.1, 1],\n",
    "            labels=['Negative', 'Neutral', 'Positive']\n",
    "        )\n",
    "        \n",
    "        # 2. Review metrics\n",
    "        df['review_length'] = df['review_text'].str.len()\n",
    "        df['word_count'] = df['review_text'].str.split().str.len()\n",
    "        \n",
    "        # 3. Skin concern detection\n",
    "        skin_concerns = {\n",
    "            'acne': ['acne', 'pimple', 'breakout', 'blemish', 'zit'],\n",
    "            'dryness': ['dry', 'flaky', 'dehydrated', 'tight'],\n",
    "            'oily': ['oily', 'greasy', 'shiny', 'sebum'],\n",
    "            'aging': ['wrinkle', 'fine line', 'aging', 'sagging', 'anti-aging'],\n",
    "            'sensitivity': ['sensitive', 'irritat', 'redness', 'burning', 'stinging'],\n",
    "            'dark_spots': ['dark spot', 'hyperpigmentation', 'discoloration', 'uneven tone'],\n",
    "            'rosacea': ['rosacea', 'redness', 'flush']\n",
    "        }\n",
    "        \n",
    "        for concern, keywords in skin_concerns.items():\n",
    "            pattern = '|'.join(keywords)\n",
    "            df[f'mentions_{concern}'] = df['review_text'].str.lower().str.contains(pattern, na=False).astype(int)\n",
    "        \n",
    "        # 4. Rating alignment (does sentiment match rating?)\n",
    "        df['rating_normalized'] = (df['rating'] - 3) / 2  # Scale to -1 to 1\n",
    "        df['sentiment_rating_alignment'] = 1 - abs(df['sentiment_score'] - df['rating_normalized'])\n",
    "        \n",
    "        # 5. Helpful review score\n",
    "        if 'helpful_votes' in df.columns:\n",
    "            df['helpful_votes'] = df['helpful_votes'].fillna(0)\n",
    "            df['helpful_score'] = np.log1p(df['helpful_votes'])\n",
    "        \n",
    "        print(f\"‚úÖ Review features extracted\")\n",
    "        print(f\"   - Avg sentiment: {df['sentiment_score'].mean():.3f}\")\n",
    "        print(f\"   - Sentiment distribution:\")\n",
    "        print(df['sentiment_category'].value_counts())\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_feature_matrices(self, products_df, reviews_df):\n",
    "        \"\"\"Create final feature matrices for ML\"\"\"\n",
    "        print(\"\\nüîß Creating feature matrices...\")\n",
    "        \n",
    "        # Products feature matrix\n",
    "        if products_df is not None:\n",
    "            product_features = products_df[[\n",
    "                'product_id', 'product_name', 'brand', 'category',\n",
    "                'ingredient_count', 'risk_score', 'risk_category',\n",
    "                'high_risk_count', 'moderate_risk_count', 'comedogenic_count',\n",
    "                'irritant_count', 'beneficial_count', 'beneficial_score'\n",
    "            ]].copy()\n",
    "            \n",
    "            if 'price' in products_df.columns:\n",
    "                product_features['price'] = products_df['price']\n",
    "            if 'rating' in products_df.columns:\n",
    "                product_features['avg_rating'] = products_df['rating']\n",
    "            \n",
    "            # Encode categorical features\n",
    "            le_brand = LabelEncoder()\n",
    "            le_category = LabelEncoder()\n",
    "            \n",
    "            product_features['brand_encoded'] = le_brand.fit_transform(product_features['brand'].fillna('unknown'))\n",
    "            product_features['category_encoded'] = le_category.fit_transform(product_features['category'].fillna('unknown'))\n",
    "            \n",
    "            # Save\n",
    "            product_features.to_csv(self.processed_dir / 'product_features.csv', index=False)\n",
    "            print(f\"‚úÖ Product features: {product_features.shape}\")\n",
    "        \n",
    "        # Reviews feature matrix\n",
    "        if reviews_df is not None:\n",
    "            review_features = reviews_df[[\n",
    "                'review_id', 'product_name', 'rating', 'sentiment_score',\n",
    "                'sentiment_category', 'review_length', 'word_count'\n",
    "            ]].copy()\n",
    "            \n",
    "            # Add skin concern columns\n",
    "            concern_cols = [col for col in reviews_df.columns if col.startswith('mentions_')]\n",
    "            for col in concern_cols:\n",
    "                review_features[col] = reviews_df[col]\n",
    "            \n",
    "            if 'helpful_score' in reviews_df.columns:\n",
    "                review_features['helpful_score'] = reviews_df['helpful_score']\n",
    "            \n",
    "            # Save\n",
    "            review_features.to_csv(self.processed_dir / 'review_features.csv', index=False)\n",
    "            print(f\"‚úÖ Review features: {review_features.shape}\")\n",
    "        \n",
    "        # Aggregate reviews by product\n",
    "        if reviews_df is not None and products_df is not None:\n",
    "            print(\"\\nüìä Aggregating reviews by product...\")\n",
    "            \n",
    "            review_aggregates = reviews_df.groupby('product_name').agg({\n",
    "                'rating': ['mean', 'count', 'std'],\n",
    "                'sentiment_score': 'mean',\n",
    "                'review_length': 'mean',\n",
    "                'word_count': 'mean'\n",
    "            }).reset_index()\n",
    "            \n",
    "            review_aggregates.columns = ['product_name', 'avg_rating', 'review_count', \n",
    "                                         'rating_std', 'avg_sentiment', 'avg_review_length',\n",
    "                                         'avg_word_count']\n",
    "            \n",
    "            # Add skin concern aggregates\n",
    "            concern_cols = [col for col in reviews_df.columns if col.startswith('mentions_')]\n",
    "            for col in concern_cols:\n",
    "                concern_agg = reviews_df.groupby('product_name')[col].sum().reset_index()\n",
    "                review_aggregates = review_aggregates.merge(concern_agg, on='product_name', how='left')\n",
    "            \n",
    "            review_aggregates.to_csv(self.processed_dir / 'review_aggregates_by_product.csv', index=False)\n",
    "            print(f\"‚úÖ Review aggregates: {review_aggregates.shape}\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Feature matrices created!\")\n",
    "    \n",
    "    def generate_data_summary_report(self):\n",
    "        \"\"\"Generate comprehensive data summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üìã DATA PREPROCESSING & FEATURE ENGINEERING SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        files = {\n",
    "            'Product Features': 'product_features.csv',\n",
    "            'Review Features': 'review_features.csv',\n",
    "            'Review Aggregates': 'review_aggregates_by_product.csv'\n",
    "        }\n",
    "        \n",
    "        for name, filename in files.items():\n",
    "            filepath = self.processed_dir / filename\n",
    "            if filepath.exists():\n",
    "                df = pd.read_csv(filepath)\n",
    "                print(f\"\\nüìä {name}:\")\n",
    "                print(f\"   Shape: {df.shape}\")\n",
    "                print(f\"   Columns: {', '.join(df.columns[:8])}...\")\n",
    "                print(f\"   File size: {filepath.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "                print(f\"   Missing values: {df.isnull().sum().sum()}\")\n",
    "            else:\n",
    "                print(f\"\\n‚ö†Ô∏è  {name}: Not found\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"‚úÖ All processing complete! Ready for ML model training.\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution pipeline\"\"\"\n",
    "    print(\"üöÄ Skincare Data Preprocessing & Feature Engineering Pipeline\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    preprocessor = SkincareDataPreprocessor()\n",
    "    \n",
    "    # Step 1: Load datasets\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 1: Loading Datasets\")\n",
    "    print(\"=\"*70)\n",
    "    datasets = preprocessor.load_datasets()\n",
    "    \n",
    "    # Step 2: Clean data\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 2: Data Cleaning\")\n",
    "    print(\"=\"*70)\n",
    "    products_clean = preprocessor.clean_products_data(datasets['products'])\n",
    "    reviews_clean = preprocessor.clean_reviews_data(datasets['reviews'])\n",
    "    \n",
    "    # Step 3: EDA\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 3: Exploratory Data Analysis\")\n",
    "    print(\"=\"*70)\n",
    "    preprocessor.perform_eda(products_clean, reviews_clean)\n",
    "    \n",
    "    # Step 4: Feature Engineering - Products\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 4: Feature Engineering - Products\")\n",
    "    print(\"=\"*70)\n",
    "    products_with_features = preprocessor.extract_ingredient_features(products_clean)\n",
    "    \n",
    "    # Step 5: Feature Engineering - Reviews\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 5: Feature Engineering - Reviews\")\n",
    "    print(\"=\"*70)\n",
    "    reviews_with_features = preprocessor.extract_review_features(reviews_clean)\n",
    "    \n",
    "    # Step 6: Create feature matrices\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 6: Creating Feature Matrices\")\n",
    "    print(\"=\"*70)\n",
    "    preprocessor.create_feature_matrices(products_with_features, reviews_with_features)\n",
    "    \n",
    "    # Step 7: Generate summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 7: Summary Report\")\n",
    "    print(\"=\"*70)\n",
    "    preprocessor.generate_data_summary_report()\n",
    "    \n",
    "    print(\"\\nüéâ Pipeline Complete!\")\n",
    "    print(\"\\nüìÇ Check the 'skincare_datasets/processed/' folder for:\")\n",
    "    print(\"   - product_features.csv\")\n",
    "    print(\"   - review_features.csv\")\n",
    "    print(\"   - review_aggregates_by_product.csv\")\n",
    "    print(\"   - visualizations/ (EDA plots)\")\n",
    "    print(\"\\nüöÄ Next: Train ML models for risk analysis and recommendations!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88d101b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Skincare ML Model Training Pipeline\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STEP 1: Loading Data\n",
      "======================================================================\n",
      "üìÇ Loading preprocessed data...\n",
      "‚úÖ Loaded products: (7878, 17)\n",
      "‚úÖ Loaded reviews: (0, 15)\n",
      "‚úÖ Loaded aggregates: (0, 14)\n",
      "\n",
      "======================================================================\n",
      "TASK 1: PRODUCT RISK CLASSIFICATION\n",
      "======================================================================\n",
      "\n",
      "üéØ Preparing Risk Classification Data...\n",
      "   Features shape: (5841, 10)\n",
      "   Target distribution:\n",
      "risk_category\n",
      "High         2428\n",
      "Low          2082\n",
      "Moderate      997\n",
      "Very High     334\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ü§ñ Training Risk Classification Models...\n",
      "======================================================================\n",
      "\n",
      "üìä Training Random Forest...\n",
      "   Accuracy: 0.9966\n",
      "   Precision: 0.9966\n",
      "   Recall: 0.9966\n",
      "   F1-Score: 0.9966\n",
      "   Error Rate: 0.0034\n",
      "   CV Score: 0.9940 (+/- 0.0029)\n",
      "\n",
      "üìä Training Gradient Boosting...\n",
      "   Accuracy: 0.9983\n",
      "   Precision: 0.9983\n",
      "   Recall: 0.9983\n",
      "   F1-Score: 0.9983\n",
      "   Error Rate: 0.0017\n",
      "   CV Score: 0.9961 (+/- 0.0014)\n",
      "\n",
      "üìä Training Logistic Regression...\n",
      "   Accuracy: 0.9940\n",
      "   Precision: 0.9940\n",
      "   Recall: 0.9940\n",
      "   F1-Score: 0.9940\n",
      "   Error Rate: 0.0060\n",
      "   CV Score: 0.9951 (+/- 0.0029)\n",
      "\n",
      "üìä Training Decision Tree...\n",
      "   Accuracy: 0.9966\n",
      "   Precision: 0.9966\n",
      "   Recall: 0.9966\n",
      "   F1-Score: 0.9966\n",
      "   Error Rate: 0.0034\n",
      "   CV Score: 0.9968 (+/- 0.0010)\n",
      "\n",
      "üìä Training K-Nearest Neighbors...\n",
      "   Accuracy: 0.9863\n",
      "   Precision: 0.9863\n",
      "   Recall: 0.9863\n",
      "   F1-Score: 0.9863\n",
      "   Error Rate: 0.0137\n",
      "   CV Score: 0.9818 (+/- 0.0045)\n",
      "\n",
      "üìä Training XGBoost...\n",
      "   ‚ö†Ô∏è  CV failed: 'super' object has no attribute '__sklearn_tags__'\n",
      "   Accuracy: 0.9974\n",
      "   Precision: 0.9975\n",
      "   Recall: 0.9974\n",
      "   F1-Score: 0.9974\n",
      "   Error Rate: 0.0026\n",
      "   CV Score: 0.9974 (+/- 0.0000)\n",
      "\n",
      "üìä Training LightGBM...\n",
      "   Accuracy: 0.9966\n",
      "   Precision: 0.9966\n",
      "   Recall: 0.9966\n",
      "   F1-Score: 0.9966\n",
      "   Error Rate: 0.0034\n",
      "   CV Score: 0.9959 (+/- 0.0014)\n",
      "\n",
      "üèÜ Best Model: Gradient Boosting (F1: 0.9983)\n",
      "\n",
      "üìà Generating confusion matrices for risk_classification...\n",
      "‚úÖ Saved: skincare_datasets\\processed\\results\\risk_classification_confusion_matrices.png\n",
      "\n",
      "üìä Generating model comparison for risk_classification...\n",
      "‚úÖ Saved: skincare_datasets\\processed\\results\\risk_classification_model_comparison.png\n",
      "\n",
      "‚ö†Ô∏è  Skipping sentiment classification (no review data)\n",
      "\n",
      "======================================================================\n",
      "TASK 3: PRODUCT RECOMMENDATION SYSTEM\n",
      "======================================================================\n",
      "\n",
      "üéØ Creating Product Recommendation System...\n",
      "‚úÖ Recommendation system created\n",
      "   Similarity matrix shape: (7878, 7878)\n",
      "\n",
      "üìã Testing Recommendation System:\n",
      "\n",
      "Original Product: Blu Mediterraneo MINIATURE Set\n",
      "Risk Score: 50.0\n",
      "Risk Category: High\n",
      "\n",
      "‚ú® Safer Alternatives:\n",
      "\n",
      "1. Facial Treatment Cleanser (SK-II)\n",
      "   Risk Score: 30.0 (‚Üì20.0)\n",
      "   Risk Category: Moderate\n",
      "   Similarity: 0.544\n",
      "\n",
      "2. Facial Treatment Cleansing Oil (SK-II)\n",
      "   Risk Score: 30.0 (‚Üì20.0)\n",
      "   Risk Category: Moderate\n",
      "   Similarity: 0.541\n",
      "\n",
      "3. Eyeshadow Palette 10 (Natasha Denona)\n",
      "   Risk Score: 30.0 (‚Üì20.0)\n",
      "   Risk Category: Moderate\n",
      "   Similarity: 0.538\n",
      "\n",
      "4. Cleanser (Eve Lom)\n",
      "   Risk Score: 40.0 (‚Üì10.0)\n",
      "   Risk Category: Moderate\n",
      "   Similarity: 0.613\n",
      "\n",
      "5. BACKSTAGE Eyeshadow Palette (Dior)\n",
      "   Risk Score: 30.0 (‚Üì20.0)\n",
      "   Risk Category: Moderate\n",
      "   Similarity: 0.511\n",
      "\n",
      "======================================================================\n",
      "STEP 4: Saving Models\n",
      "======================================================================\n",
      "\n",
      "üíæ Saving models...\n",
      "‚úÖ Saved risk_classifier\n",
      "‚úÖ Saved risk_scaler\n",
      "‚úÖ Saved risk_encoder\n",
      "\n",
      "======================================================================\n",
      "üìã FINAL MODEL PERFORMANCE REPORT\n",
      "======================================================================\n",
      "\n",
      "üéØ RISK CLASSIFICATION MODELS:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Random Forest:\n",
      "  Accuracy:  0.9966 (99.66%)\n",
      "  Precision: 0.9966\n",
      "  Recall:    0.9966\n",
      "  F1-Score:  0.9966\n",
      "  Error Rate: 0.0034 (0.34%)\n",
      "  CV Score:  0.9940 (+/- 0.0029)\n",
      "\n",
      "Gradient Boosting:\n",
      "  Accuracy:  0.9983 (99.83%)\n",
      "  Precision: 0.9983\n",
      "  Recall:    0.9983\n",
      "  F1-Score:  0.9983\n",
      "  Error Rate: 0.0017 (0.17%)\n",
      "  CV Score:  0.9961 (+/- 0.0014)\n",
      "\n",
      "Logistic Regression:\n",
      "  Accuracy:  0.9940 (99.40%)\n",
      "  Precision: 0.9940\n",
      "  Recall:    0.9940\n",
      "  F1-Score:  0.9940\n",
      "  Error Rate: 0.0060 (0.60%)\n",
      "  CV Score:  0.9951 (+/- 0.0029)\n",
      "\n",
      "Decision Tree:\n",
      "  Accuracy:  0.9966 (99.66%)\n",
      "  Precision: 0.9966\n",
      "  Recall:    0.9966\n",
      "  F1-Score:  0.9966\n",
      "  Error Rate: 0.0034 (0.34%)\n",
      "  CV Score:  0.9968 (+/- 0.0010)\n",
      "\n",
      "K-Nearest Neighbors:\n",
      "  Accuracy:  0.9863 (98.63%)\n",
      "  Precision: 0.9863\n",
      "  Recall:    0.9863\n",
      "  F1-Score:  0.9863\n",
      "  Error Rate: 0.0137 (1.37%)\n",
      "  CV Score:  0.9818 (+/- 0.0045)\n",
      "\n",
      "XGBoost:\n",
      "  Accuracy:  0.9974 (99.74%)\n",
      "  Precision: 0.9975\n",
      "  Recall:    0.9974\n",
      "  F1-Score:  0.9974\n",
      "  Error Rate: 0.0026 (0.26%)\n",
      "  CV Score:  0.9974 (+/- 0.0000)\n",
      "\n",
      "LightGBM:\n",
      "  Accuracy:  0.9966 (99.66%)\n",
      "  Precision: 0.9966\n",
      "  Recall:    0.9966\n",
      "  F1-Score:  0.9966\n",
      "  Error Rate: 0.0034 (0.34%)\n",
      "  CV Score:  0.9959 (+/- 0.0014)\n",
      "\n",
      "======================================================================\n",
      "‚úÖ All models trained and evaluated successfully!\n",
      "======================================================================\n",
      "\n",
      "üéâ ML Pipeline Complete!\n",
      "\n",
      "üìÇ Check these folders:\n",
      "   - Models: skincare_datasets\\processed\\models\n",
      "   - Results: skincare_datasets\\processed\\results\n",
      "\n",
      "üöÄ Next Steps:\n",
      "   1. Review the confusion matrices and model comparisons\n",
      "   2. Test the recommendation system with different products\n",
      "   3. Integrate the trained models into your web application\n",
      "   4. Use the saved .pkl files for making predictions\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Comprehensive ML Models for Skincare Risk Analysis & Recommendation System\n",
    "Fixed for scikit-learn compatibility issues\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_recall_fscore_support, roc_auc_score, roc_curve,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "\n",
    "# Classification Models\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier,\n",
    "    AdaBoostClassifier, VotingClassifier\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Try importing XGBoost and LightGBM (optional)\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  XGBoost not available, using alternatives\")\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  LightGBM not available, using alternatives\")\n",
    "\n",
    "# Recommendation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "class SkincareMLPipeline:\n",
    "    def __init__(self, data_dir='skincare_datasets/processed'):\n",
    "        \"\"\"Initialize ML pipeline\"\"\"\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.models_dir = self.data_dir / 'models'\n",
    "        self.models_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.results_dir = self.data_dir / 'results'\n",
    "        self.results_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.encoders = {}\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load preprocessed datasets\"\"\"\n",
    "        print(\"üìÇ Loading preprocessed data...\")\n",
    "        \n",
    "        datasets = {}\n",
    "        files = {\n",
    "            'products': 'product_features.csv',\n",
    "            'reviews': 'review_features.csv',\n",
    "            'aggregates': 'review_aggregates_by_product.csv'\n",
    "        }\n",
    "        \n",
    "        for name, filename in files.items():\n",
    "            filepath = self.data_dir / filename\n",
    "            if filepath.exists():\n",
    "                datasets[name] = pd.read_csv(filepath)\n",
    "                print(f\"‚úÖ Loaded {name}: {datasets[name].shape}\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  {name} not found\")\n",
    "                datasets[name] = None\n",
    "        \n",
    "        return datasets\n",
    "    \n",
    "    def prepare_risk_classification_data(self, products_df):\n",
    "        \"\"\"Prepare data for risk classification (Multi-class)\"\"\"\n",
    "        print(\"\\nüéØ Preparing Risk Classification Data...\")\n",
    "        \n",
    "        df = products_df.copy()\n",
    "        \n",
    "        # Features for prediction\n",
    "        feature_cols = [\n",
    "            'ingredient_count', 'high_risk_count', 'moderate_risk_count',\n",
    "            'comedogenic_count', 'irritant_count', 'beneficial_count',\n",
    "            'beneficial_score'\n",
    "        ]\n",
    "        \n",
    "        # Add encoded features if available\n",
    "        if 'brand_encoded' in df.columns:\n",
    "            feature_cols.append('brand_encoded')\n",
    "        if 'category_encoded' in df.columns:\n",
    "            feature_cols.append('category_encoded')\n",
    "        if 'price' in df.columns:\n",
    "            df['price'] = df['price'].fillna(df['price'].median())\n",
    "            feature_cols.append('price')\n",
    "        \n",
    "        # Remove rows with missing target\n",
    "        df = df[df['risk_category'].notna()].copy()\n",
    "        \n",
    "        X = df[feature_cols].fillna(0)\n",
    "        y = df['risk_category']\n",
    "        \n",
    "        # Encode target\n",
    "        le = LabelEncoder()\n",
    "        y_encoded = le.fit_transform(y)\n",
    "        \n",
    "        print(f\"   Features shape: {X.shape}\")\n",
    "        print(f\"   Target distribution:\")\n",
    "        print(df['risk_category'].value_counts())\n",
    "        \n",
    "        return X, y_encoded, le, feature_cols\n",
    "    \n",
    "    def prepare_sentiment_classification_data(self, reviews_df):\n",
    "        \"\"\"Prepare data for review sentiment classification\"\"\"\n",
    "        print(\"\\nüí¨ Preparing Sentiment Classification Data...\")\n",
    "        \n",
    "        df = reviews_df.copy()\n",
    "        \n",
    "        # Check if sentiment_category column exists\n",
    "        if 'sentiment_category' not in df.columns:\n",
    "            print(\"   ‚ö†Ô∏è  'sentiment_category' column not found!\")\n",
    "            print(\"   Available columns:\", df.columns.tolist())\n",
    "            return None, None, None, None\n",
    "        \n",
    "        # Remove rows with missing sentiment\n",
    "        df = df[df['sentiment_category'].notna()].copy()\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            print(\"   ‚ö†Ô∏è  No valid sentiment data found after filtering!\")\n",
    "            return None, None, None, None\n",
    "        \n",
    "        feature_cols = ['rating', 'review_length', 'word_count']\n",
    "        \n",
    "        # Add skin concern features\n",
    "        concern_cols = [col for col in df.columns if col.startswith('mentions_')]\n",
    "        feature_cols.extend(concern_cols)\n",
    "        \n",
    "        if 'helpful_score' in df.columns:\n",
    "            feature_cols.append('helpful_score')\n",
    "        \n",
    "        # Keep only columns that exist\n",
    "        feature_cols = [col for col in feature_cols if col in df.columns]\n",
    "        \n",
    "        if len(feature_cols) == 0:\n",
    "            print(\"   ‚ö†Ô∏è  No valid feature columns found!\")\n",
    "            return None, None, None, None\n",
    "        \n",
    "        X = df[feature_cols].fillna(0)\n",
    "        y = df['sentiment_category']\n",
    "        \n",
    "        # Encode target\n",
    "        le = LabelEncoder()\n",
    "        y_encoded = le.fit_transform(y)\n",
    "        \n",
    "        print(f\"   Features shape: {X.shape}\")\n",
    "        print(f\"   Features used: {feature_cols}\")\n",
    "        print(f\"   Target distribution:\")\n",
    "        print(df['sentiment_category'].value_counts())\n",
    "        \n",
    "        return X, y_encoded, le, feature_cols\n",
    "    \n",
    "    def train_risk_classification_models(self, X, y, label_encoder):\n",
    "        \"\"\"Train multiple models for risk classification\"\"\"\n",
    "        print(\"\\nü§ñ Training Risk Classification Models...\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        self.scalers['risk_scaler'] = scaler\n",
    "        \n",
    "        # Define models (compatible versions)\n",
    "        models = {\n",
    "            'Random Forest': RandomForestClassifier(\n",
    "                n_estimators=200, max_depth=15, min_samples_split=5,\n",
    "                random_state=42, n_jobs=-1\n",
    "            ),\n",
    "            'Gradient Boosting': GradientBoostingClassifier(\n",
    "                n_estimators=150, max_depth=7, learning_rate=0.1,\n",
    "                random_state=42\n",
    "            ),\n",
    "            'Logistic Regression': LogisticRegression(\n",
    "                max_iter=1000, random_state=42, n_jobs=-1\n",
    "            ),\n",
    "            'Decision Tree': DecisionTreeClassifier(\n",
    "                max_depth=15, min_samples_split=5, random_state=42\n",
    "            ),\n",
    "            'K-Nearest Neighbors': KNeighborsClassifier(\n",
    "                n_neighbors=5, n_jobs=-1\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Add XGBoost if available\n",
    "        if XGBOOST_AVAILABLE:\n",
    "            models['XGBoost'] = XGBClassifier(\n",
    "                n_estimators=200, max_depth=8, learning_rate=0.1,\n",
    "                random_state=42, n_jobs=-1, \n",
    "                eval_metric='mlogloss',\n",
    "                use_label_encoder=False\n",
    "            )\n",
    "        \n",
    "        # Add LightGBM if available\n",
    "        if LIGHTGBM_AVAILABLE:\n",
    "            models['LightGBM'] = LGBMClassifier(\n",
    "                n_estimators=200, max_depth=8, learning_rate=0.1,\n",
    "                random_state=42, n_jobs=-1, verbose=-1\n",
    "            )\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            print(f\"\\nüìä Training {name}...\")\n",
    "            \n",
    "            try:\n",
    "                # Train\n",
    "                if name in ['Logistic Regression', 'K-Nearest Neighbors']:\n",
    "                    model.fit(X_train_scaled, y_train)\n",
    "                    y_pred = model.predict(X_test_scaled)\n",
    "                    if hasattr(model, 'predict_proba'):\n",
    "                        y_pred_proba = model.predict_proba(X_test_scaled)\n",
    "                    else:\n",
    "                        y_pred_proba = None\n",
    "                    X_cv = X_train_scaled\n",
    "                else:\n",
    "                    model.fit(X_train, y_train)\n",
    "                    y_pred = model.predict(X_test)\n",
    "                    if hasattr(model, 'predict_proba'):\n",
    "                        y_pred_proba = model.predict_proba(X_test)\n",
    "                    else:\n",
    "                        y_pred_proba = None\n",
    "                    X_cv = X_train\n",
    "                \n",
    "                # Evaluate\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                    y_test, y_pred, average='weighted', zero_division=0\n",
    "                )\n",
    "                \n",
    "                # Cross-validation\n",
    "                try:\n",
    "                    cv_scores = cross_val_score(model, X_cv, y_train, cv=5)\n",
    "                    cv_mean = cv_scores.mean()\n",
    "                    cv_std = cv_scores.std()\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  CV failed: {str(e)[:50]}\")\n",
    "                    cv_mean = accuracy\n",
    "                    cv_std = 0.0\n",
    "                \n",
    "                results[name] = {\n",
    "                    'model': model,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1_score': f1,\n",
    "                    'cv_mean': cv_mean,\n",
    "                    'cv_std': cv_std,\n",
    "                    'y_test': y_test,\n",
    "                    'y_pred': y_pred,\n",
    "                    'y_pred_proba': y_pred_proba,\n",
    "                    'error_rate': 1 - accuracy\n",
    "                }\n",
    "                \n",
    "                print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "                print(f\"   Precision: {precision:.4f}\")\n",
    "                print(f\"   Recall: {recall:.4f}\")\n",
    "                print(f\"   F1-Score: {f1:.4f}\")\n",
    "                print(f\"   Error Rate: {1-accuracy:.4f}\")\n",
    "                print(f\"   CV Score: {cv_mean:.4f} (+/- {cv_std:.4f})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Failed to train {name}: {str(e)[:100]}\")\n",
    "                continue\n",
    "        \n",
    "        if not results:\n",
    "            raise ValueError(\"No models were successfully trained!\")\n",
    "        \n",
    "        # Find best model\n",
    "        best_model_name = max(results, key=lambda x: results[x]['f1_score'])\n",
    "        print(f\"\\nüèÜ Best Model: {best_model_name} (F1: {results[best_model_name]['f1_score']:.4f})\")\n",
    "        \n",
    "        # Save best model\n",
    "        self.models['risk_classifier'] = results[best_model_name]['model']\n",
    "        self.encoders['risk_encoder'] = label_encoder\n",
    "        \n",
    "        return results, best_model_name\n",
    "    \n",
    "    def train_sentiment_classification_models(self, X, y, label_encoder):\n",
    "        \"\"\"Train models for sentiment classification\"\"\"\n",
    "        print(\"\\nü§ñ Training Sentiment Classification Models...\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        self.scalers['sentiment_scaler'] = scaler\n",
    "        \n",
    "        # Define models\n",
    "        models = {\n",
    "            'Random Forest': RandomForestClassifier(\n",
    "                n_estimators=150, max_depth=12, random_state=42, n_jobs=-1\n",
    "            ),\n",
    "            'Logistic Regression': LogisticRegression(\n",
    "                max_iter=1000, random_state=42, n_jobs=-1\n",
    "            ),\n",
    "            'Naive Bayes': GaussianNB(),\n",
    "            'Gradient Boosting': GradientBoostingClassifier(\n",
    "                n_estimators=100, max_depth=5, random_state=42\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Add XGBoost if available\n",
    "        if XGBOOST_AVAILABLE:\n",
    "            models['XGBoost'] = XGBClassifier(\n",
    "                n_estimators=150, max_depth=6, learning_rate=0.1,\n",
    "                random_state=42, n_jobs=-1, \n",
    "                eval_metric='mlogloss',\n",
    "                use_label_encoder=False\n",
    "            )\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            print(f\"\\nüìä Training {name}...\")\n",
    "            \n",
    "            try:\n",
    "                # Train\n",
    "                if name in ['Logistic Regression', 'Naive Bayes']:\n",
    "                    model.fit(X_train_scaled, y_train)\n",
    "                    y_pred = model.predict(X_test_scaled)\n",
    "                else:\n",
    "                    model.fit(X_train, y_train)\n",
    "                    y_pred = model.predict(X_test)\n",
    "                \n",
    "                # Evaluate\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                    y_test, y_pred, average='weighted', zero_division=0\n",
    "                )\n",
    "                \n",
    "                results[name] = {\n",
    "                    'model': model,\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1_score': f1,\n",
    "                    'y_test': y_test,\n",
    "                    'y_pred': y_pred,\n",
    "                    'error_rate': 1 - accuracy\n",
    "                }\n",
    "                \n",
    "                print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "                print(f\"   F1-Score: {f1:.4f}\")\n",
    "                print(f\"   Error Rate: {1-accuracy:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Failed to train {name}: {str(e)[:100]}\")\n",
    "                continue\n",
    "        \n",
    "        if not results:\n",
    "            raise ValueError(\"No models were successfully trained!\")\n",
    "        \n",
    "        # Best model\n",
    "        best_model_name = max(results, key=lambda x: results[x]['f1_score'])\n",
    "        print(f\"\\nüèÜ Best Model: {best_model_name} (F1: {results[best_model_name]['f1_score']:.4f})\")\n",
    "        \n",
    "        self.models['sentiment_classifier'] = results[best_model_name]['model']\n",
    "        self.encoders['sentiment_encoder'] = label_encoder\n",
    "        \n",
    "        return results, best_model_name\n",
    "    \n",
    "    def plot_confusion_matrices(self, results, task_name, label_encoder):\n",
    "        \"\"\"Plot confusion matrices for all models\"\"\"\n",
    "        print(f\"\\nüìà Generating confusion matrices for {task_name}...\")\n",
    "        \n",
    "        n_models = len(results)\n",
    "        cols = min(3, n_models)\n",
    "        rows = (n_models + cols - 1) // cols\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(6*cols, 5*rows))\n",
    "        if n_models == 1:\n",
    "            axes = np.array([axes])\n",
    "        axes = axes.flatten() if n_models > 1 else axes\n",
    "        \n",
    "        for idx, (name, result) in enumerate(results.items()):\n",
    "            cm = confusion_matrix(result['y_test'], result['y_pred'])\n",
    "            \n",
    "            # Plot\n",
    "            sns.heatmap(\n",
    "                cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=label_encoder.classes_,\n",
    "                yticklabels=label_encoder.classes_,\n",
    "                ax=axes[idx], cbar=True\n",
    "            )\n",
    "            \n",
    "            axes[idx].set_title(f'{name}\\nAcc: {result[\"accuracy\"]:.4f} | Err: {result[\"error_rate\"]:.4f}')\n",
    "            axes[idx].set_ylabel('True Label')\n",
    "            axes[idx].set_xlabel('Predicted Label')\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for idx in range(n_models, len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = self.results_dir / f'{task_name}_confusion_matrices.png'\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"‚úÖ Saved: {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_model_comparison(self, results, task_name):\n",
    "        \"\"\"Plot model performance comparison\"\"\"\n",
    "        print(f\"\\nüìä Generating model comparison for {task_name}...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        models = list(results.keys())\n",
    "        accuracies = [results[m]['accuracy'] for m in models]\n",
    "        precisions = [results[m]['precision'] for m in models]\n",
    "        recalls = [results[m]['recall'] for m in models]\n",
    "        f1_scores = [results[m]['f1_score'] for m in models]\n",
    "        error_rates = [results[m]['error_rate'] for m in models]\n",
    "        \n",
    "        # Create comparison plots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # 1. Accuracy comparison\n",
    "        axes[0, 0].barh(models, accuracies, color='steelblue', alpha=0.8)\n",
    "        axes[0, 0].set_xlabel('Accuracy')\n",
    "        axes[0, 0].set_title('Model Accuracy Comparison')\n",
    "        axes[0, 0].set_xlim(0, 1)\n",
    "        for i, v in enumerate(accuracies):\n",
    "            axes[0, 0].text(v + 0.01, i, f'{v:.4f}', va='center')\n",
    "        \n",
    "        # 2. Error Rate comparison\n",
    "        axes[0, 1].barh(models, error_rates, color='crimson', alpha=0.8)\n",
    "        axes[0, 1].set_xlabel('Error Rate')\n",
    "        axes[0, 1].set_title('Model Error Rate Comparison')\n",
    "        axes[0, 1].set_xlim(0, max(error_rates) * 1.2 if error_rates else 1)\n",
    "        for i, v in enumerate(error_rates):\n",
    "            axes[0, 1].text(v + 0.005, i, f'{v:.4f}', va='center')\n",
    "        \n",
    "        # 3. Precision, Recall, F1 comparison\n",
    "        x = np.arange(len(models))\n",
    "        width = 0.25\n",
    "        \n",
    "        axes[1, 0].bar(x - width, precisions, width, label='Precision', alpha=0.8)\n",
    "        axes[1, 0].bar(x, recalls, width, label='Recall', alpha=0.8)\n",
    "        axes[1, 0].bar(x + width, f1_scores, width, label='F1-Score', alpha=0.8)\n",
    "        axes[1, 0].set_ylabel('Score')\n",
    "        axes[1, 0].set_title('Precision, Recall, F1-Score Comparison')\n",
    "        axes[1, 0].set_xticks(x)\n",
    "        axes[1, 0].set_xticklabels(models, rotation=45, ha='right')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].set_ylim(0, 1)\n",
    "        axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # 4. Overall metrics table\n",
    "        axes[1, 1].axis('off')\n",
    "        table_data = []\n",
    "        for model in models:\n",
    "            table_data.append([\n",
    "                model,\n",
    "                f\"{results[model]['accuracy']:.4f}\",\n",
    "                f\"{results[model]['precision']:.4f}\",\n",
    "                f\"{results[model]['recall']:.4f}\",\n",
    "                f\"{results[model]['f1_score']:.4f}\",\n",
    "                f\"{results[model]['error_rate']:.4f}\"\n",
    "            ])\n",
    "        \n",
    "        table = axes[1, 1].table(\n",
    "            cellText=table_data,\n",
    "            colLabels=['Model', 'Accuracy', 'Precision', 'Recall', 'F1', 'Error'],\n",
    "            cellLoc='center',\n",
    "            loc='center',\n",
    "            colWidths=[0.25, 0.15, 0.15, 0.15, 0.15, 0.15]\n",
    "        )\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(9)\n",
    "        table.scale(1, 2)\n",
    "        \n",
    "        # Style header\n",
    "        for i in range(6):\n",
    "            table[(0, i)].set_facecolor('#40466e')\n",
    "            table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "        \n",
    "        # Highlight best values\n",
    "        best_idx = accuracies.index(max(accuracies))\n",
    "        for i in range(6):\n",
    "            table[(best_idx + 1, i)].set_facecolor('#90EE90')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = self.results_dir / f'{task_name}_model_comparison.png'\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"‚úÖ Saved: {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def create_product_recommendation_system(self, products_df):\n",
    "        \"\"\"Create content-based recommendation system\"\"\"\n",
    "        print(\"\\nüéØ Creating Product Recommendation System...\")\n",
    "        \n",
    "        df = products_df.copy()\n",
    "        \n",
    "        # Create feature matrix for similarity\n",
    "        feature_cols = [\n",
    "            'ingredient_count', 'risk_score', 'beneficial_score',\n",
    "            'high_risk_count', 'moderate_risk_count', 'comedogenic_count'\n",
    "        ]\n",
    "        \n",
    "        if 'category_encoded' in df.columns:\n",
    "            feature_cols.append('category_encoded')\n",
    "        if 'price' in df.columns:\n",
    "            df['price'] = df['price'].fillna(df['price'].median())\n",
    "            feature_cols.append('price')\n",
    "        \n",
    "        # Normalize features\n",
    "        scaler = StandardScaler()\n",
    "        features_scaled = scaler.fit_transform(df[feature_cols].fillna(0))\n",
    "        \n",
    "        # Calculate similarity matrix\n",
    "        similarity_matrix = cosine_similarity(features_scaled)\n",
    "        \n",
    "        print(f\"‚úÖ Recommendation system created\")\n",
    "        print(f\"   Similarity matrix shape: {similarity_matrix.shape}\")\n",
    "        \n",
    "        return similarity_matrix, df\n",
    "    \n",
    "    def recommend_safer_alternatives(self, product_idx, similarity_matrix, products_df, n=5):\n",
    "        \"\"\"Recommend safer alternative products\"\"\"\n",
    "        \n",
    "        # Get product details\n",
    "        product = products_df.iloc[product_idx]\n",
    "        product_risk = product['risk_score']\n",
    "        \n",
    "        # Find similar products with lower risk\n",
    "        similarities = similarity_matrix[product_idx]\n",
    "        \n",
    "        # Create recommendation scores\n",
    "        rec_scores = []\n",
    "        for i, sim in enumerate(similarities):\n",
    "            if i != product_idx:\n",
    "                other_risk = products_df.iloc[i]['risk_score']\n",
    "                if other_risk < product_risk:\n",
    "                    rec_score = sim * (1 - (other_risk / 100))\n",
    "                    rec_scores.append((i, sim, rec_score))\n",
    "        \n",
    "        # Sort by recommendation score\n",
    "        rec_scores.sort(key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        # Get top N recommendations\n",
    "        recommendations = []\n",
    "        for i, sim, rec_score in rec_scores[:n]:\n",
    "            rec_product = products_df.iloc[i]\n",
    "            recommendations.append({\n",
    "                'product_name': rec_product['product_name'],\n",
    "                'brand': rec_product['brand'],\n",
    "                'risk_score': rec_product['risk_score'],\n",
    "                'risk_category': rec_product['risk_category'],\n",
    "                'similarity': sim,\n",
    "                'recommendation_score': rec_score,\n",
    "                'risk_reduction': product_risk - rec_product['risk_score']\n",
    "            })\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def save_models(self):\n",
    "        \"\"\"Save all trained models\"\"\"\n",
    "        print(\"\\nüíæ Saving models...\")\n",
    "        \n",
    "        # Save models\n",
    "        for name, model in self.models.items():\n",
    "            filename = self.models_dir / f'{name}.pkl'\n",
    "            with open(filename, 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "            print(f\"‚úÖ Saved {name}\")\n",
    "        \n",
    "        # Save scalers\n",
    "        for name, scaler in self.scalers.items():\n",
    "            filename = self.models_dir / f'{name}.pkl'\n",
    "            with open(filename, 'wb') as f:\n",
    "                pickle.dump(scaler, f)\n",
    "            print(f\"‚úÖ Saved {name}\")\n",
    "        \n",
    "        # Save encoders\n",
    "        for name, encoder in self.encoders.items():\n",
    "            filename = self.models_dir / f'{name}.pkl'\n",
    "            with open(filename, 'wb') as f:\n",
    "                pickle.dump(encoder, f)\n",
    "            print(f\"‚úÖ Saved {name}\")\n",
    "    \n",
    "    def generate_final_report(self, risk_results, sentiment_results):\n",
    "        \"\"\"Generate comprehensive performance report\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üìã FINAL MODEL PERFORMANCE REPORT\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\nüéØ RISK CLASSIFICATION MODELS:\")\n",
    "        print(\"-\" * 70)\n",
    "        for name, result in risk_results.items():\n",
    "            print(f\"\\n{name}:\")\n",
    "            print(f\"  Accuracy:  {result['accuracy']:.4f} ({result['accuracy']*100:.2f}%)\")\n",
    "            print(f\"  Precision: {result['precision']:.4f}\")\n",
    "            print(f\"  Recall:    {result['recall']:.4f}\")\n",
    "            print(f\"  F1-Score:  {result['f1_score']:.4f}\")\n",
    "            print(f\"  Error Rate: {result['error_rate']:.4f} ({result['error_rate']*100:.2f}%)\")\n",
    "            if 'cv_mean' in result:\n",
    "                print(f\"  CV Score:  {result['cv_mean']:.4f} (+/- {result['cv_std']:.4f})\")\n",
    "        \n",
    "        if sentiment_results:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"\\nüí¨ SENTIMENT CLASSIFICATION MODELS:\")\n",
    "            print(\"-\" * 70)\n",
    "            for name, result in sentiment_results.items():\n",
    "                print(f\"\\n{name}:\")\n",
    "                print(f\"  Accuracy:  {result['accuracy']:.4f} ({result['accuracy']*100:.2f}%)\")\n",
    "                print(f\"  Precision: {result['precision']:.4f}\")\n",
    "                print(f\"  Recall:    {result['recall']:.4f}\")\n",
    "                print(f\"  F1-Score:  {result['f1_score']:.4f}\")\n",
    "                print(f\"  Error Rate: {result['error_rate']:.4f} ({result['error_rate']*100:.2f}%)\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"‚úÖ All models trained and evaluated successfully!\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution pipeline\"\"\"\n",
    "    print(\"üöÄ Skincare ML Model Training Pipeline\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = SkincareMLPipeline()\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 1: Loading Data\")\n",
    "    print(\"=\"*70)\n",
    "    datasets = pipeline.load_data()\n",
    "    \n",
    "    if datasets['products'] is None:\n",
    "        print(\"‚ùå Error: Product data not found. Run preprocessing first!\")\n",
    "        return\n",
    "    \n",
    "    # === TASK 1: RISK CLASSIFICATION ===\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TASK 1: PRODUCT RISK CLASSIFICATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    X_risk, y_risk, le_risk, risk_features = pipeline.prepare_risk_classification_data(\n",
    "        datasets['products']\n",
    "    )\n",
    "    \n",
    "    risk_results, best_risk_model = pipeline.train_risk_classification_models(\n",
    "        X_risk, y_risk, le_risk\n",
    "    )\n",
    "    \n",
    "    pipeline.plot_confusion_matrices(risk_results, 'risk_classification', le_risk)\n",
    "    pipeline.plot_model_comparison(risk_results, 'risk_classification')\n",
    "    \n",
    "    # === TASK 2: SENTIMENT CLASSIFICATION ===\n",
    "    sentiment_results = {}\n",
    "    if datasets['reviews'] is not None and len(datasets['reviews']) > 0:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"TASK 2: REVIEW SENTIMENT CLASSIFICATION\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        X_sentiment, y_sentiment, le_sentiment, sentiment_features = \\\n",
    "            pipeline.prepare_sentiment_classification_data(datasets['reviews'])\n",
    "        \n",
    "        # Only train if we have valid data (all must be not None)\n",
    "        if (X_sentiment is not None and y_sentiment is not None and \n",
    "            le_sentiment is not None and sentiment_features is not None and \n",
    "            len(X_sentiment) > 0):\n",
    "            \n",
    "            sentiment_results, best_sentiment_model = pipeline.train_sentiment_classification_models(\n",
    "                X_sentiment, y_sentiment, le_sentiment\n",
    "            )\n",
    "            \n",
    "            if sentiment_results:  # Check if training was successful\n",
    "                pipeline.plot_confusion_matrices(sentiment_results, 'sentiment_classification', le_sentiment)\n",
    "                pipeline.plot_model_comparison(sentiment_results, 'sentiment_classification')\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Skipping sentiment classification (invalid/empty data)\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Skipping sentiment classification (no review data)\")\n",
    "    \n",
    "    # === TASK 3: RECOMMENDATION SYSTEM ===\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TASK 3: PRODUCT RECOMMENDATION SYSTEM\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    similarity_matrix, products_for_rec = pipeline.create_product_recommendation_system(\n",
    "        datasets['products']\n",
    "    )\n",
    "    \n",
    "    # Test recommendation\n",
    "    print(\"\\nüìã Testing Recommendation System:\")\n",
    "    test_idx = 0\n",
    "    test_product = products_for_rec.iloc[test_idx]\n",
    "    print(f\"\\nOriginal Product: {test_product['product_name']}\")\n",
    "    print(f\"Risk Score: {test_product['risk_score']:.1f}\")\n",
    "    print(f\"Risk Category: {test_product['risk_category']}\")\n",
    "    \n",
    "    recommendations = pipeline.recommend_safer_alternatives(\n",
    "        test_idx, similarity_matrix, products_for_rec, n=5\n",
    "    )\n",
    "    \n",
    "    if recommendations:\n",
    "        print(\"\\n‚ú® Safer Alternatives:\")\n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            print(f\"\\n{i}. {rec['product_name']} ({rec['brand']})\")\n",
    "            print(f\"   Risk Score: {rec['risk_score']:.1f} (‚Üì{rec['risk_reduction']:.1f})\")\n",
    "            print(f\"   Risk Category: {rec['risk_category']}\")\n",
    "            print(f\"   Similarity: {rec['similarity']:.3f}\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No safer alternatives found for this product.\")\n",
    "    \n",
    "    # Save models\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 4: Saving Models\")\n",
    "    print(\"=\"*70)\n",
    "    pipeline.save_models()\n",
    "    \n",
    "    # Final report\n",
    "    pipeline.generate_final_report(risk_results, sentiment_results)\n",
    "    \n",
    "    print(\"\\nüéâ ML Pipeline Complete!\")\n",
    "    print(\"\\nüìÇ Check these folders:\")\n",
    "    print(f\"   - Models: {pipeline.models_dir}\")\n",
    "    print(f\"   - Results: {pipeline.results_dir}\")\n",
    "    print(\"\\nüöÄ Next Steps:\")\n",
    "    print(\"   1. Review the confusion matrices and model comparisons\")\n",
    "    print(\"   2. Test the recommendation system with different products\")\n",
    "    print(\"   3. Integrate the trained models into your web application\")\n",
    "    print(\"   4. Use the saved .pkl files for making predictions\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b96d888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
